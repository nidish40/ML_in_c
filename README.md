# ğŸ“¦ ML in C â€” Custom Machine Learning Framework

This project is a from-scratch implementation of a **minimal machine learning framework written in C**, designed to understand how ML models work internally without relying on external libraries.

The framework builds up from matrix operations â†’ regression models â†’ activation functions â†’ loss functions â†’ neural network components.

This README describes **everything completed so far**.

---

## âœ… Current Features Implemented

---

## ğŸ§® 1. Matrix Module

A custom `Matrix` type with all the core operations required for ML calculations.

### âœ” Completed:
- Matrix creation & deletion  
- Addition, subtraction, multiplication  
- Transpose  
- Element-wise operations  
- Shape validation  
- Debug printing  

These form the foundation for all models.

---

## ğŸ§  2. Regression Models

### âœ” **Linear Regression**
- Forward pass (`y = XW + b`)  
- MSE loss  
- Gradient computation  
- Training via Gradient Descent  

### âœ” **Logistic Regression**
- Sigmoid activation  
- Binary cross-entropy loss  
- Gradient updates  
- Predicting probabilities & binary classes  

These give you a complete understanding of classical ML models before neural networks.

---

## ğŸ”Œ 3. Activation Functions

Fully implemented activation functions used in neural networks.

### âœ” Completed:
- ReLU  
- Sigmoid  
- Tanh  
- Softmax (numerically stable, subtract-max trick)

---

## ğŸ“‰ 4. Loss Functions

All loss functions also return gradients for backpropagation.

### âœ” **Mean Squared Error (MSE)**
Used in regression.  
Measures squared difference between predictions and targets.

### âœ” **Cross Entropy (from probabilities)**
Used for classification when softmax probabilities are already computed.

### âœ” **Softmax Cross Entropy (with logits)**
Recommended version.  
Computes softmax + cross-entropy + gradient in a single, numerically stable step.

---

## ğŸ—ï¸ 5. Current Project Status

So far, the following are fully functional:
- Core matrix engine  
- Linear regression  
- Logistic regression  
- Activation functions  
- Loss functions + gradients  
- Softmax & numerical stability handling  

The foundation is now ready for building neural network layers and backpropagation.

---



